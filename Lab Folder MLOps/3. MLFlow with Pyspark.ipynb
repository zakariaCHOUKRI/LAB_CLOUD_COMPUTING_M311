{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0e9cb8",
   "metadata": {},
   "source": [
    "## MLFlow avec Pyspark\n",
    "\n",
    "Avant de commencer, installez Spark en local en suivant le guide \"Spark Installation.pdf\"\n",
    "\n",
    "Nous allons suivre la même démarche que nous avons utilisé dans le premier notebook. \n",
    "\n",
    "Nous allons utilisé aussi le même dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04271821",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: A JNI error has occurred, please check your installation and try again\n",
      "Exception in thread \"main\" java.lang.UnsupportedClassVersionError: org/apache/spark/launcher/Main has been compiled by a more recent version of the Java Runtime (class file version 61.0), this version of the Java Runtime only recognizes class file versions up to 52.0\n",
      "\tat java.lang.ClassLoader.defineClass1(Native Method)\n",
      "\tat java.lang.ClassLoader.defineClass(ClassLoader.java:756)\n",
      "\tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n",
      "\tat java.net.URLClassLoader.defineClass(URLClassLoader.java:473)\n",
      "\tat java.net.URLClassLoader.access$100(URLClassLoader.java:74)\n",
      "\tat java.net.URLClassLoader$1.run(URLClassLoader.java:369)\n",
      "\tat java.net.URLClassLoader$1.run(URLClassLoader.java:363)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:362)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n",
      "\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n",
      "\tat sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:621)\n",
      "/home/zczak/LAB_CLOUD_COMPUTING_M311/Lab Folder MLOps/venv/lib/python3.12/site-packages/pyspark/bin/spark-class: line 97: CMD: bad array subscript\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkRuntimeError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspark\u001b[39;00m\n\u001b[32m     18\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mSPARK_LOCAL_IP\u001b[39m\u001b[33m\"\u001b[39m]=\u001b[33m'\u001b[39m\u001b[33m127.0.0.1\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocal[*]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m spark.sparkContext._conf.getAll()\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpyspark: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(pyspark.__version__))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LAB_CLOUD_COMPUTING_M311/Lab Folder MLOps/venv/lib/python3.12/site-packages/pyspark/sql/session.py:556\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    559\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LAB_CLOUD_COMPUTING_M311/Lab Folder MLOps/venv/lib/python3.12/site-packages/pyspark/core/context.py:523\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LAB_CLOUD_COMPUTING_M311/Lab Folder MLOps/venv/lib/python3.12/site-packages/pyspark/core/context.py:205\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    208\u001b[39m         master,\n\u001b[32m    209\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m         memory_profiler_cls,\n\u001b[32m    220\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LAB_CLOUD_COMPUTING_M311/Lab Folder MLOps/venv/lib/python3.12/site-packages/pyspark/core/context.py:444\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LAB_CLOUD_COMPUTING_M311/Lab Folder MLOps/venv/lib/python3.12/site-packages/pyspark/java_gateway.py:111\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    108\u001b[39m     time.sleep(\u001b[32m0.1\u001b[39m)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    112\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m         messageParameters={},\n\u001b[32m    114\u001b[39m     )\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[32m    117\u001b[39m     gateway_port = read_int(info)\n",
      "\u001b[31mPySparkRuntimeError\u001b[39m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "import pyspark #\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "import seaborn as sns\n",
    "import sklearn #\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import matplotlib #\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "os.environ[\"SPARK_LOCAL_IP\"]='127.0.0.1'\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "spark.sparkContext._conf.getAll()\n",
    "print(\"pyspark: {}\".format(pyspark.__version__))\n",
    "print(\"matplotlib: {}\".format(matplotlib.__version__))\n",
    "print(\"seaborn: {}\".format(sns.__version__))\n",
    "print(\"sklearn: {}\".format(sklearn.__version__))\n",
    "print(\"mlflow: {}\".format(mlflow.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41f1895",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/creditcard.csv'\n",
    "df = spark.read.csv(data_path, header = True,\n",
    "inferSchema = True)\n",
    "labelColumn = \"Class\"\n",
    "columns = df.columns\n",
    "numericCols = columns\n",
    "numericCols.remove(\"Time\")\n",
    "numericCols.remove(labelColumn)\n",
    "print(numericCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7250ef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0be770",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "assemblerInputs = numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs,\n",
    "outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "dfFeatures = df.select(F.col(labelColumn).alias('label'),*numericCols )\n",
    "normal = dfFeatures.filter(\"Class == 0\").sample(withReplacement=False, fraction=0.5, seed=2020)\n",
    "anomaly = dfFeatures.filter(\"Class == 1\")\n",
    "normal_train, normal_test = normal.randomSplit([0.8, 0.2],seed = 2020)\n",
    "anomaly_train, anomaly_test = anomaly.randomSplit([0.8, 0.2],seed = 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e408d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFeatures.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdd524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = normal_train.union(anomaly_train)\n",
    "test_set = normal_test.union(anomaly_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75009c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(dfFeatures)\n",
    "train_set = pipelineModel.transform(train_set)\n",
    "test_set = pipelineModel.transform(test_set)\n",
    "selectedCols = ['label', 'features'] + numericCols\n",
    "train_set = train_set.select(selectedCols)\n",
    "test_set = test_set.select(selectedCols)\n",
    "print(\"Training Dataset Count: \", train_set.count())\n",
    "print(\"Test Dataset Count: \", test_set.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd590f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(spark_model, train_set):\n",
    "    trained_model = spark_model.fit(train_set)\n",
    "    trainingSummary = trained_model.summary\n",
    "    pyspark_auc_score = trainingSummary.areaUnderROC\n",
    "    mlflow.log_metric(\"train_acc\", trainingSummary.accuracy)\n",
    "    mlflow.log_metric(\"train_AUC\", pyspark_auc_score)\n",
    "    print(\"Training Accuracy: \", trainingSummary.accuracy)\n",
    "    print(\"Training AUC:\", pyspark_auc_score)\n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c79071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(spark_model, test_set):\n",
    "    evaluation_summary = spark_model.evaluate(test_set)\n",
    "    eval_acc = evaluation_summary.accuracy\n",
    "    eval_AUC = evaluation_summary.areaUnderROC\n",
    "    mlflow.log_metric(\"eval_acc\", eval_acc)\n",
    "    mlflow.log_metric(\"eval_AUC\", eval_AUC)\n",
    "    print(\"Evaluation Accuracy: \", eval_acc)\n",
    "    print(\"Evaluation AUC: \", eval_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b23638",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol = 'features', labelCol ='label', maxIter=10)\n",
    "mlflow.set_experiment(\"PySpark_CreditCard\")\n",
    "with mlflow.start_run():\n",
    "    trainedLR = train(lr, train_set)\n",
    "    evaluate(trainedLR, test_set)\n",
    "    mlflow.spark.log_model(trainedLR,\"creditcard_model_pyspark\")\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5588b788",
   "metadata": {},
   "source": [
    "## Loading the model\n",
    "\n",
    "copiez l'ID run a partir de l'interface MLFlow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2612bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlflow.spark.load_model(\"runs:/votre_RUN_ID/creditcard_model_pyspark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9549b62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_set)\n",
    "y_true = predictions.select(['label']).collect()\n",
    "y_pred = predictions.select(['prediction']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78915eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"AUC Score: {roc_auc_score(y_true, y_pred):.3%}\")\n",
    "print(f\"Accuracy Score: {accuracy_score(y_true, y_pred):.3%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7458d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "ax = sns.heatmap(conf_matrix, annot=True,fmt='g')\n",
    "ax.invert_xaxis()\n",
    "ax.invert_yaxis()\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8c48d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eb8787",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
